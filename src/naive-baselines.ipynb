{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-12T22:39:05.028869Z",
     "start_time": "2024-02-12T22:38:59.135510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/418718 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c60d4462f09243e0b60783afc209f41b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/104928 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c99625ff6930405d907f8ce7e7eb5b39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/11138 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "954fae9a2dba430cbd9ca3887bf081d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/11940 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9658be408a6548c582ae20d7c3c55fed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/7356 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bd368bae2ff4a6ebf42a0e2af64bad1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/984 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a2095789d8441b89730a13cafd83bb8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/972 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f671cb5c6d2e4fbeacc79df134d82688"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset('lecslab/glosslm-split')\n",
    "dataset = dataset.filter(lambda x: x[\"transcription\"] is not None and x[\"glosses\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from eval import strip_gloss_punctuation\n",
    "\n",
    "# Segmented\n",
    "# For each language, learn the conditional distribution of morphemes -> glosses\n",
    "\n",
    "all_train = datasets.concatenate_datasets([dataset['train'], dataset['train_OOD']])\n",
    "all_train = all_train.filter(lambda row: row[\"is_segmented\"] == \"yes\")\n",
    "all_test = datasets.concatenate_datasets([dataset['test_ID'], dataset['test_OOD']])\n",
    "all_test = all_test.filter(lambda row: row[\"is_segmented\"] == \"yes\")\n",
    "\n",
    "def gloss_with_top_gloss(gloss_dict):\n",
    "    return max(gloss_dict, key=gloss_dict.get)\n",
    "\n",
    "def gloss_with_random_gloss(gloss_dict):\n",
    "    return random.choice(list(gloss_dict.keys()))\n",
    "\n",
    "\n",
    "def make_predictions(glottocode, method):\n",
    "    select_gloss = {'top': gloss_with_top_gloss, 'random': gloss_with_random_gloss}[method]\n",
    "\n",
    "    train_data = all_train.filter(lambda row: row['glottocode'] == glottocode)\n",
    "    test_data = all_test.filter(lambda row: row['glottocode'] == glottocode)\n",
    "\n",
    "    morpheme_glosses = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for row in train_data:\n",
    "        for word, glossed_word in zip(strip_gloss_punctuation(row['transcription']).split(),\n",
    "                                      strip_gloss_punctuation(row['glosses']).split()):\n",
    "            for morpheme, gloss in zip(re.split(r\"\\s|-\", word), re.split(r\"\\s|-\", glossed_word)):\n",
    "                morpheme_glosses[morpheme.lower()][gloss] += 1\n",
    "\n",
    "\n",
    "    preds = []\n",
    "    for row in test_data:\n",
    "        line_predictions = []\n",
    "        for word in strip_gloss_punctuation(row['transcription']).split():\n",
    "            word_predictions = []\n",
    "            for morpheme in re.split(r\"\\s|-\", word):\n",
    "                if morpheme not in morpheme_glosses:\n",
    "                    word_predictions.append(\"???\")\n",
    "                else:\n",
    "                    word_predictions.append(select_gloss(morpheme_glosses[morpheme.lower()]))\n",
    "            line_predictions.append('-'.join(word_predictions))\n",
    "        preds.append(' '.join(line_predictions))\n",
    "\n",
    "    gold = [strip_gloss_punctuation(g) for g in test_data[\"glosses\"]]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"id\": test_data[\"id\"],\n",
    "        \"glottocode\": test_data[\"glottocode\"],\n",
    "        \"is_segmented\": test_data[\"is_segmented\"],\n",
    "        \"pred\": preds,\n",
    "        \"gold\": gold,\n",
    "    })\n",
    "\n",
    "\n",
    "splits = {'ID': ['arap1274', 'dido1241', 'uspa1245'],\n",
    "          'OOD': ['gitx1241', 'lezg1247', 'natu1246', 'nyan1302' ]}\n",
    "for method in ['top', 'random']:\n",
    "    for split in ['ID', 'OOD']:\n",
    "        all_preds = []\n",
    "        for lang in splits[split]:\n",
    "            all_preds.append(make_predictions(lang, method))\n",
    "\n",
    "        combined = pd.concat(all_preds)\n",
    "        combined.to_csv(f'../preds/naive-{method}/test_{split}-preds.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T00:54:55.265348Z",
     "start_time": "2024-02-02T00:54:47.359514Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'transcription': 'ɑʑ-ɑd',\n 'glosses': 'front-POSS.2SG.INE/ILL',\n 'translation': 'in front of you',\n 'glottocode': 'udmu1245',\n 'id': 'uratyp_124',\n 'source': 'uratyp',\n 'metalang_glottocode': 'stan1293',\n 'is_segmented': 'yes',\n 'language': 'Udmurt',\n 'metalang': 'English'}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T23:52:48.043945Z",
     "start_time": "2024-02-01T23:52:48.041076Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/188006 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34f9e457d1dc4913920ef82980f0d0b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/6456 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "730f031d823d4856a0967452bc59ebe6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/188006 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a78c2f578c3141f3a273f0d798eb5671"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/6456 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e305cac19d747378bd3fbc9b5039282"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/188006 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2be41819f0f14ead9e24e42f234f29a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/6456 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8da4c595e2e7491d8fc6345bcc27fc81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/188006 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f20af554f0d4641a8dba4ddc0caf874"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/6456 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "434d59fb41e4478593feb123bd840078"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from eval import strip_gloss_punctuation\n",
    "\n",
    "# Segmented\n",
    "# For each language, learn the conditional distribution of morphemes -> glosses\n",
    "\n",
    "all_train = datasets.concatenate_datasets([dataset['train'], dataset['train_OOD']])\n",
    "all_train = all_train.filter(lambda row: row[\"is_segmented\"] == \"no\")\n",
    "all_test = datasets.concatenate_datasets([dataset['test_ID'], dataset['test_OOD']])\n",
    "all_test = all_test.filter(lambda row: row[\"is_segmented\"] == \"no\")\n",
    "\n",
    "def gloss_with_top_gloss(gloss_dict):\n",
    "    return max(gloss_dict, key=gloss_dict.get)\n",
    "\n",
    "def gloss_with_random_gloss(gloss_dict):\n",
    "    return random.choice(list(gloss_dict.keys()))\n",
    "\n",
    "\n",
    "def make_predictions(glottocode, method):\n",
    "    select_gloss = {'top': gloss_with_top_gloss, 'random': gloss_with_random_gloss}[method]\n",
    "\n",
    "    train_data = all_train.filter(lambda row: row['glottocode'] == glottocode)\n",
    "    test_data = all_test.filter(lambda row: row['glottocode'] == glottocode)\n",
    "\n",
    "    morpheme_glosses = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for row in train_data:\n",
    "        for word, glossed_word in zip(strip_gloss_punctuation(row['transcription']).split(),\n",
    "                                      strip_gloss_punctuation(row['glosses']).split()):\n",
    "            morpheme_glosses[word.lower()][glossed_word] += 1\n",
    "\n",
    "\n",
    "    preds = []\n",
    "    for row in test_data:\n",
    "        line_predictions = []\n",
    "        for word in strip_gloss_punctuation(row['transcription']).split():\n",
    "            if word not in morpheme_glosses:\n",
    "                word_prediction = \"???\"\n",
    "            else:\n",
    "                word_prediction = select_gloss(morpheme_glosses[word.lower()])\n",
    "            line_predictions.append(word_prediction)\n",
    "        preds.append(' '.join(line_predictions))\n",
    "\n",
    "    gold = [strip_gloss_punctuation(g) for g in test_data[\"glosses\"]]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"id\": test_data[\"id\"],\n",
    "        \"glottocode\": test_data[\"glottocode\"],\n",
    "        \"is_segmented\": test_data[\"is_segmented\"],\n",
    "        \"pred\": preds,\n",
    "        \"gold\": gold,\n",
    "    })\n",
    "\n",
    "\n",
    "splits = {'ID': ['arap1274', 'dido1241', 'uspa1245'],\n",
    "          'OOD': ['gitx1241', 'lezg1247', 'natu1246', 'nyan1302' ]}\n",
    "for method in ['top', 'random']:\n",
    "    for split in ['ID', 'OOD']:\n",
    "        all_preds = []\n",
    "        for lang in splits[split]:\n",
    "            all_preds.append(make_predictions(lang, method))\n",
    "\n",
    "        combined = pd.concat(all_preds)\n",
    "        combined.to_csv(f'../preds/naive-{method}-unseg/test_{split}-preds.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T22:41:08.043549Z",
     "start_time": "2024-02-12T22:40:50.263483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
