{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b720a8f9-a47b-4a1f-bdf1-432beaf77a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milesper/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from st_data.data import IGTLine\n",
    "\n",
    "def load_data_file(path: str):\n",
    "    \"\"\"Loads a file containing IGT data into a list of entries.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # If we have a directory, recursively load all files and concat together\n",
    "    if os.path.isdir(path):\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                all_data.extend(load_data_file(os.path.join(path, file)))\n",
    "        return all_data\n",
    "\n",
    "    # If we have one file, read in line by line\n",
    "    with open(path, 'r') as file:\n",
    "        current_entry = [None, None, None, None]  # transc, segm, gloss, transl\n",
    "\n",
    "        skipped_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            # Determine the type of line\n",
    "            # If we see a type that has already been filled for the current entry, something is wrong\n",
    "            line_prefix = line[:2]\n",
    "            if line_prefix == '\\\\t' and current_entry[0] == None:\n",
    "                current_entry[0] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\m' and current_entry[1] == None:\n",
    "                current_entry[1] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\g' and current_entry[2] == None:\n",
    "                if len(line[3:].strip()) > 0:\n",
    "                    current_entry[2] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\l' and current_entry[3] == None:\n",
    "                current_entry[3] = line[3:].strip()\n",
    "                # Once we have the translation, we've reached the end and can save this entry\n",
    "                all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                        segmentation=current_entry[1],\n",
    "                                        glosses=current_entry[2],\n",
    "                                        translation=current_entry[3]))\n",
    "                current_entry = [None, None, None, None]\n",
    "            elif line_prefix == \"\\\\p\":\n",
    "                # Skip POS lines\n",
    "                continue\n",
    "            elif line.strip() != \"\":\n",
    "                # Something went wrong\n",
    "                skipped_lines.append(line)\n",
    "                continue\n",
    "            else:\n",
    "                if not current_entry == [None, None, None, None]:\n",
    "                    all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                            segmentation=current_entry[1],\n",
    "                                            glosses=current_entry[2],\n",
    "                                            translation=None))\n",
    "                    current_entry = [None, None, None, None]\n",
    "        # Might have one extra line at the end\n",
    "        if not current_entry == [None, None, None, None]:\n",
    "            all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                    segmentation=current_entry[1],\n",
    "                                    glosses=current_entry[2],\n",
    "                                    translation=None))\n",
    "        print(f\"Skipped {len(skipped_lines)} lines\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86ca3e0-cd23-45af-af01-63b55a4720a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 752/752 [00:00<00:00, 4.13MB/s]\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/28.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  15%|█▍        | 4.19M/28.0M [00:00<00:04, 4.92MB/s]\u001b[A\n",
      "Downloading data:  45%|████▍     | 12.6M/28.0M [00:01<00:01, 9.35MB/s]\u001b[A\n",
      "Downloading data:  75%|███████▍  | 21.0M/28.0M [00:02<00:00, 11.5MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 28.0M/28.0M [00:02<00:00, 12.5MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.25s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 623.78it/s]\n",
      "Generating train split: 100%|██████████| 425020/425020 [00:00<00:00, 2740366.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "glosslm = datasets.load_dataset(\"lecslab/glosslm\", download_mode='force_redownload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8dcff95-bb8f-4f10-a68e-a79b3c6a0a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./st_data/splits/Arapaho\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/splits/Gitksan\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/splits/Lezgi\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/splits/Natugu\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/splits/Nyangbo\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/splits/Tsez\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/splits/Uspanteko\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n"
     ]
    }
   ],
   "source": [
    "def create_hf_dataset(filename, isocode, glottocode, metalang):\n",
    "    print(f\"Loading {filename}\")\n",
    "    train_data = load_data_file(filename + f\"/{isocode}-train-track2-uncovered\")\n",
    "    dev_data = load_data_file(filename + f\"/{isocode}-dev-track2-uncovered\")\n",
    "    test_data = load_data_file(filename + f\"/{isocode}-test-track2-uncovered\")\n",
    "    \n",
    "    def parse_data(raw_data, id_prefix: str):\n",
    "        data = []\n",
    "        for i, line in enumerate(raw_data):\n",
    "            new_row = {'glottocode': glottocode, 'metalang_glottocode': metalang, \"is_segmented\": \"yes\", \"source\": \"sigmorphon_st\", \"type\": \"canonical\"}\n",
    "            new_row['ID'] = f\"st_{id_prefix}_{glottocode}_{i}\"\n",
    "            new_row['transcription'] = line.segmentation\n",
    "            new_row['glosses'] = line.glosses\n",
    "            new_row['translation'] = line.translation\n",
    "            data.append(new_row)\n",
    "\n",
    "            new_row_unsegmented = {'glottocode': glottocode, 'metalang_glottocode': metalang, \"is_segmented\": \"no\", \"source\": \"sigmorphon_st\", \"type\": \"canonical\"}\n",
    "            new_row_unsegmented['ID'] = f\"st_{id_prefix}_{glottocode}_{i}\"\n",
    "            new_row_unsegmented['transcription'] = line.transcription\n",
    "            new_row_unsegmented['glosses'] = line.glosses\n",
    "            new_row_unsegmented['translation'] = line.translation\n",
    "            data.append(new_row_unsegmented)\n",
    "        return data\n",
    "    \n",
    "    data = parse_data(train_data, 'train') + parse_data(dev_data, 'dev') + parse_data(test_data, 'test')\n",
    "\n",
    "    return datasets.Dataset.from_list(data)\n",
    "\n",
    "st_data = {\n",
    "    \"arp\": create_hf_dataset(\"./st_data/splits/Arapaho\", \"arp\", \"arap1274\", \"stan1293\"),\n",
    "    \"git\": create_hf_dataset(\"./st_data/splits/Gitksan\", \"git\", \"gitx1241\", \"stan1293\"),\n",
    "    \"lez\": create_hf_dataset(\"./st_data/splits/Lezgi\", \"lez\", \"lezg1247\", \"stan1293\"),\n",
    "    \"nat\": create_hf_dataset(\"./st_data/splits/Natugu\", \"ntu\", \"natu1246\", \"stan1293\"),\n",
    "    \"nyb\": create_hf_dataset(\"./st_data/splits/Nyangbo\", \"nyb\", \"nyan1302\", \"stan1293\"),\n",
    "    \"ddo\": create_hf_dataset(\"./st_data/splits/Tsez\", \"ddo\", \"dido1241\", \"stan1293\"),\n",
    "    \"usp\": create_hf_dataset(\"./st_data/splits/Uspanteko\", \"usp\", \"uspa1245\", \"stan1288\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "835491af-14d3-4209-8b15-ffc2709a37b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arp': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 97832\n",
       " }),\n",
       " 'git': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 220\n",
       " }),\n",
       " 'lez': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 1752\n",
       " }),\n",
       " 'nat': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 1978\n",
       " }),\n",
       " 'nyb': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 5252\n",
       " }),\n",
       " 'ddo': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 8896\n",
       " }),\n",
       " 'usp': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 21278\n",
       " })}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da42bce8-907d-4d30-a947-d618f1a4112e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7382cd6eebe4d2c9e53f581b48c639b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7840180de5ef4343a7b2718a4ae9c8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3578d0df0be476b841318f31ce963c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "# combined = datasets.concatenate_datasets([glosslm['train']] + list(st_data.values()))\n",
    "# combined.push_to_hub(\"lecslab/glosslm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d82cbfb-5c82-4f0a-9963-164884f94209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/426 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  23%|██▎       | 100/426 [00:00<00:00, 999.02ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  58%|█████▊    | 246/426 [00:00<00:00, 1268.54ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 426/426 [00:00<00:00, 1271.09ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.21s/it]\n",
      "README.md: 100%|██████████| 752/752 [00:00<00:00, 2.68MB/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace data on hf\n",
    "\n",
    "st_data_rows = datasets.concatenate_datasets(list(st_data.values())).to_pandas()\n",
    "\n",
    "old_data = glosslm['train'].to_pandas()\n",
    "old_data = old_data[old_data['source'] != \"sigmorphon_st\"]\n",
    "a = pd.concat([old_data, st_data_rows])\n",
    "ds = datasets.Dataset.from_pandas(a).remove_columns([\"__index_level_0__\"])\n",
    "ds.push_to_hub(\"lecslab/glosslm\", commit_message='Fix ST data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f1e00-9665-495e-95c3-7d0be7536081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
