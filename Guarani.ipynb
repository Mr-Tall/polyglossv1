{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19dc3f02-ab6b-47b7-ac21-d2c314addba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def convert_html_to_igt(filename):\n",
    "    # Initialize lists to store extracted data with more robust filtering\n",
    "    robust_transcriptions = []\n",
    "    robust_glosses = []\n",
    "    robust_translations = []\n",
    "\n",
    "    # Read the HTML file\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all tables and corresponding glosses\n",
    "    tables = soup.find_all('table')\n",
    "    gloss_divs = soup.find_all('div', {'class': 'gloss'})\n",
    "\n",
    "    # More robustly filter out gloss divs that do not contain actual translations\n",
    "    robust_gloss_divs = [div for div in gloss_divs if div.find('p') and 'style' in div.find('p').attrs and 'color' in div.find('p')['style']]\n",
    "\n",
    "    # Iterate through each table and robustly filtered gloss div\n",
    "    for table, gloss_div in zip(tables, robust_gloss_divs):\n",
    "        # Extract rows from the table\n",
    "        rows = table.find_all('tr')\n",
    "\n",
    "        # Extract the transcription and gloss from the table, if they exist\n",
    "        if len(rows) >= 2:\n",
    "            transcription_cells = rows[0].find_all('td')\n",
    "            \n",
    "            # Delete tooltips\n",
    "            for tooltip in rows[0].find_all(\"span\", {'class':'tooltip'}): \n",
    "                tooltip.decompose()\n",
    "            \n",
    "            gloss_cells = rows[1].find_all('td')\n",
    "\n",
    "            transcription = ' '.join(cell.text.strip() for cell in transcription_cells)\n",
    "            gloss = ' '.join(cell.text.strip() for cell in gloss_cells)\n",
    "        else:\n",
    "            transcription = ''\n",
    "            gloss = ''\n",
    "\n",
    "        # Extract the translation from the gloss div\n",
    "        translation = gloss_div.find('p').text.strip()\n",
    "\n",
    "        # Append to the lists\n",
    "        robust_transcriptions.append(transcription)\n",
    "        robust_glosses.append(gloss)\n",
    "        robust_translations.append(translation)\n",
    "    \n",
    "    with open(filename[:-4] + 'txt', 'w') as f:\n",
    "        for (transc, gloss, transl) in zip(robust_transcriptions, robust_glosses, robust_translations):\n",
    "            f.write('\\n\\n\\\\t ' + transc)\n",
    "            f.write('\\n\\n\\\\m ' + transc)\n",
    "            f.write('\\n\\\\g ' + gloss)\n",
    "            f.write('\\n\\\\l ' + transl)\n",
    "\n",
    "convert_html_to_igt(f'./Guarani Corpus/Story1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d876aaa-a86c-4082-bbaf-d49c9a8c7a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1, 16):\n",
    "    convert_html_to_igt(f'./Guarani Corpus/Story{i}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2aed7a-6665-42e9-ad63-0ebf899cc30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Fix segmentation/transcription lines\n",
    "\n",
    "for file in os.listdir('./Guarani Corpus'):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join('./Guarani Corpus', file), 'r') as f:\n",
    "            new_lines = []\n",
    "            for line in f:\n",
    "                line_prefix = line[:2]\n",
    "                if line_prefix == '\\\\t':\n",
    "                    transr = line[3:]\n",
    "                    transr_unseg = transr.replace(\"-\", \"\")\n",
    "                    new_lines.append(\"\\\\t \" + transr_unseg)\n",
    "                    new_lines.append(\"\\\\m \" + transr)\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "                    \n",
    "            # Write fixed file\n",
    "            with open(os.path.join('./Guarani Corpus', file[:-4] + '-fixed.txt'), 'w') as wf:\n",
    "                wf.write(\"\".join(new_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dda45c3-fe77-440f-b54b-8518c589b5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milesper/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./Guarani Corpus/data-fixed\n",
      "Story3-fixed.txt\n",
      "Looks good\n",
      "Story12-fixed.txt\n",
      "Looks good\n",
      "Story8-fixed.txt\n",
      "Looks good\n",
      "Story14-fixed.txt\n",
      "Looks good\n",
      "Story5-fixed.txt\n",
      "Looks good\n",
      "Story2-fixed.txt\n",
      "Looks good\n",
      "Story13-fixed.txt\n",
      "Looks good\n",
      "Story9-fixed.txt\n",
      "Looks good\n",
      "Story15-fixed.txt\n",
      "Looks good\n",
      "Story4-fixed.txt\n",
      "Looks good\n",
      "Story1-fixed.txt\n",
      "Looks good\n",
      "Story10-fixed.txt\n",
      "Looks good\n",
      "Story7-fixed.txt\n",
      "Looks good\n",
      "Story11-fixed.txt\n",
      "Looks good\n",
      "Story6-fixed.txt\n",
      "Looks good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "    num_rows: 1606\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from st_data.data import IGTLine, create_hf_dataset\n",
    "\n",
    "guarani_data = create_hf_dataset(\"./Guarani Corpus/data-fixed\", \"para1311\", \"stan1293\", row_id=\"guarani\")\n",
    "guarani_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7bbffe0-f255-41c3-a3b3-c3191c9cda70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oguahẽ ndaje ka’i karai jaguarete róga-pe , o-jerure haguã posáda .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_punc(text):\n",
    "    return text.replace('\\t', ' ') \\\n",
    "            .replace(\"?\", \" ?\") \\\n",
    "            .replace(\".\", \" .\") \\\n",
    "            .replace(\"!\", \" !\") \\\n",
    "            .replace(\",\", \" ,\") \n",
    "\n",
    "tokenize_punc(\"Oguahẽ ndaje ka’i karai jaguarete róga-pe, o-jerure haguã posáda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e73409-aacf-425e-a2d2-5e3ff845002d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1606/1606 [00:00<00:00, 20516.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def fix_data(row):\n",
    "    row[\"transcription\"] = tokenize_punc(row[\"transcription\"])\n",
    "    row[\"source\"] = \"guarani\"\n",
    "    return row\n",
    "\n",
    "guarani_data = guarani_data.map(fix_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db35f75f-57d4-422d-b592-54e12c98fe10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb7cb0be6504e2c8a8d05d0611323d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3c160490d54d55a324ce98138e25e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/744 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guarani_data.push_to_hub(\"lecslab/guarani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce08a02b-21f6-4ee2-8355-58c15dd9a720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 745/745 [00:00<00:00, 1.69MB/s]\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/27.9M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  15%|█▌        | 4.19M/27.9M [00:00<00:03, 6.21MB/s]\u001b[A\n",
      "Downloading data:  45%|████▌     | 12.6M/27.9M [00:01<00:01, 10.9MB/s]\u001b[A\n",
      "Downloading data:  75%|███████▌  | 21.0M/27.9M [00:01<00:00, 12.2MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 27.9M/27.9M [00:02<00:00, 13.1MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 343.29it/s]\n",
      "Generating train split: 100%|██████████| 423414/423414 [00:00<00:00, 2460093.91 examples/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/426 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  31%|███       | 133/426 [00:00<00:00, 1328.82ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  62%|██████▏   | 266/426 [00:00<00:00, 1188.87ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 426/426 [00:00<00:00, 1311.82ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n",
      "README.md: 100%|██████████| 745/745 [00:00<00:00, 2.61MB/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# run this cell to replace the data on HF\n",
    "old_data = datasets.load_dataset(\"lecslab/glosslm\", download_mode='force_redownload')['train']\n",
    "\n",
    "combined = datasets.concatenate_datasets([old_data, guarani_data])\n",
    "\n",
    "combined.push_to_hub(\"lecslab/glosslm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a77f527-d5ac-4b96-ad5e-b57129edf309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'glottocode', 'transcription', 'glosses', 'translation', 'metalang_glottocode', 'is_segmented', 'source', 'type'],\n",
       "    num_rows: 425020\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02b4ca70-c9ae-4ad0-aabd-a8e2927b3972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'glottocode', 'transcription', 'glosses', 'translation', 'metalang_glottocode', 'is_segmented', 'source', 'type'],\n",
       "    num_rows: 423414\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf09b5-0121-4bdc-9fc3-312f021b4434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
