{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19dc3f02-ab6b-47b7-ac21-d2c314addba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def convert_html_to_igt(filename):\n",
    "    # Initialize lists to store extracted data with more robust filtering\n",
    "    robust_transcriptions = []\n",
    "    robust_glosses = []\n",
    "    robust_translations = []\n",
    "\n",
    "    # Read the HTML file\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all tables and corresponding glosses\n",
    "    tables = soup.find_all('table')\n",
    "    gloss_divs = soup.find_all('div', {'class': 'gloss'})\n",
    "\n",
    "    # More robustly filter out gloss divs that do not contain actual translations\n",
    "    robust_gloss_divs = [div for div in gloss_divs if div.find('p') and 'style' in div.find('p').attrs and 'color' in div.find('p')['style']]\n",
    "\n",
    "    # Iterate through each table and robustly filtered gloss div\n",
    "    for table, gloss_div in zip(tables, robust_gloss_divs):\n",
    "        # Extract rows from the table\n",
    "        rows = table.find_all('tr')\n",
    "\n",
    "        # Extract the transcription and gloss from the table, if they exist\n",
    "        if len(rows) >= 2:\n",
    "            transcription_cells = rows[0].find_all('td')\n",
    "            \n",
    "            # Delete tooltips\n",
    "            for tooltip in rows[0].find_all(\"span\", {'class':'tooltip'}): \n",
    "                tooltip.decompose()\n",
    "            \n",
    "            gloss_cells = rows[1].find_all('td')\n",
    "\n",
    "            transcription = ' '.join(cell.text.strip() for cell in transcription_cells)\n",
    "            gloss = ' '.join(cell.text.strip() for cell in gloss_cells)\n",
    "        else:\n",
    "            transcription = ''\n",
    "            gloss = ''\n",
    "\n",
    "        # Extract the translation from the gloss div\n",
    "        translation = gloss_div.find('p').text.strip()\n",
    "\n",
    "        # Append to the lists\n",
    "        robust_transcriptions.append(transcription)\n",
    "        robust_glosses.append(gloss)\n",
    "        robust_translations.append(translation)\n",
    "    \n",
    "    with open(filename[:-4] + 'txt', 'w') as f:\n",
    "        for (transc, gloss, transl) in zip(robust_transcriptions, robust_glosses, robust_translations):\n",
    "            f.write('\\n\\n\\\\t ' + transc)\n",
    "            f.write('\\n\\n\\\\m ' + transc)\n",
    "            f.write('\\n\\\\g ' + gloss)\n",
    "            f.write('\\n\\\\l ' + transl)\n",
    "\n",
    "convert_html_to_igt(f'./Guarani Corpus/Story1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d876aaa-a86c-4082-bbaf-d49c9a8c7a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1, 16):\n",
    "    convert_html_to_igt(f'./Guarani Corpus/Story{i}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c2aed7a-6665-42e9-ad63-0ebf899cc30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Fix segmentation/transcription lines\n",
    "\n",
    "for file in os.listdir('./Guarani Corpus'):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join('./Guarani Corpus', file), 'r') as f:\n",
    "            new_lines = []\n",
    "            for line in f:\n",
    "                line_prefix = line[:2]\n",
    "                if line_prefix == '\\\\t':\n",
    "                    transr = line[3:]\n",
    "                    transr_unseg = transr.replace(\"-\", \"\")\n",
    "                    new_lines.append(\"\\\\t \" + transr_unseg)\n",
    "                    new_lines.append(\"\\\\m \" + transr)\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "                    \n",
    "            # Write fixed file\n",
    "            with open(os.path.join('./Guarani Corpus', file[:-4] + '-fixed.txt'), 'w') as wf:\n",
    "                wf.write(\"\".join(new_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dda45c3-fe77-440f-b54b-8518c589b5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./Guarani Corpus/data-fixed\n",
      "Story3-fixed.txt\n",
      "Looks good\n",
      "Story12-fixed.txt\n",
      "Looks good\n",
      "Story8-fixed.txt\n",
      "Looks good\n",
      "Story14-fixed.txt\n",
      "Looks good\n",
      "Story5-fixed.txt\n",
      "Looks good\n",
      "Story2-fixed.txt\n",
      "Looks good\n",
      "Story13-fixed.txt\n",
      "Looks good\n",
      "Story9-fixed.txt\n",
      "Looks good\n",
      "Story15-fixed.txt\n",
      "Looks good\n",
      "Story4-fixed.txt\n",
      "Looks good\n",
      "Story1-fixed.txt\n",
      "Looks good\n",
      "Story10-fixed.txt\n",
      "Looks good\n",
      "Story7-fixed.txt\n",
      "Looks good\n",
      "Story11-fixed.txt\n",
      "Looks good\n",
      "Story6-fixed.txt\n",
      "Looks good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "    num_rows: 1606\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from st_data.data import IGTLine, create_hf_dataset\n",
    "\n",
    "guarani_data = create_hf_dataset(\"./Guarani Corpus/data-fixed\", \"para1311\", \"stan1293\", row_id=\"guarani\")\n",
    "guarani_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bbffe0-f255-41c3-a3b3-c3191c9cda70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oguahẽ ndaje ka’i karai jaguarete róga-pe , o-jerure haguã posáda .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_punc(text):\n",
    "    return text.replace('\\t', ' ') \\\n",
    "            .replace(\"?\", \" ?\") \\\n",
    "            .replace(\".\", \" .\") \\\n",
    "            .replace(\"!\", \" !\") \\\n",
    "            .replace(\",\", \" ,\") \n",
    "\n",
    "tokenize_punc(\"Oguahẽ ndaje ka’i karai jaguarete róga-pe, o-jerure haguã posáda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e73409-aacf-425e-a2d2-5e3ff845002d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebe467dc4d6434fb07d005845a6bf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fix_data(row):\n",
    "    row[\"transcription\"] = tokenize_punc(row[\"transcription\"])\n",
    "    row[\"source\"] = \"guarani\"\n",
    "    return row\n",
    "\n",
    "guarani_data = guarani_data.map(fix_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db35f75f-57d4-422d-b592-54e12c98fe10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875a8b841a7a49cbb53082ba857e8f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af31179f92b462f8d3aad66edb3ab99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guarani_data.push_to_hub(\"lecslab/guarani\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce08a02b-21f6-4ee2-8355-58c15dd9a720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5c724e78ad40a0b2ae277dd8734629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c7fc93932d471badc078a55a6df7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/454 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301b07c4d6e240aebf7c608cd0f5b809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3ae2fa668440eebfe52db80940291b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# run this cell to replace the data on HF\n",
    "old_data = datasets.load_dataset(\"lecslab/glosslm\")['train']\n",
    "\n",
    "combined = datasets.concatenate_datasets([old_data, guarani_data])\n",
    "\n",
    "combined.push_to_hub(\"lecslab/glosslm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77f527-d5ac-4b96-ad5e-b57129edf309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
