{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b720a8f9-a47b-4a1f-bdf1-432beaf77a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from st_data.data import IGTLine\n",
    "\n",
    "def load_data_file(path: str):\n",
    "    \"\"\"Loads a file containing IGT data into a list of entries.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # If we have a directory, recursively load all files and concat together\n",
    "    if os.path.isdir(path):\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                all_data.extend(load_data_file(os.path.join(path, file)))\n",
    "        return all_data\n",
    "\n",
    "    # If we have one file, read in line by line\n",
    "    with open(path, 'r') as file:\n",
    "        current_entry = [None, None, None, None]  # transc, segm, gloss, transl\n",
    "\n",
    "        skipped_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            # Determine the type of line\n",
    "            # If we see a type that has already been filled for the current entry, something is wrong\n",
    "            line_prefix = line[:2]\n",
    "            if line_prefix == '\\\\t' and current_entry[0] == None:\n",
    "                current_entry[0] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\m' and current_entry[1] == None:\n",
    "                current_entry[1] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\g' and current_entry[2] == None:\n",
    "                if len(line[3:].strip()) > 0:\n",
    "                    current_entry[2] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\l' and current_entry[3] == None:\n",
    "                current_entry[3] = line[3:].strip()\n",
    "                # Once we have the translation, we've reached the end and can save this entry\n",
    "                all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                        segmentation=current_entry[1],\n",
    "                                        glosses=current_entry[2],\n",
    "                                        translation=current_entry[3]))\n",
    "                current_entry = [None, None, None, None]\n",
    "            elif line_prefix == \"\\\\p\":\n",
    "                # Skip POS lines\n",
    "                continue\n",
    "            elif line.strip() != \"\":\n",
    "                # Something went wrong\n",
    "                skipped_lines.append(line)\n",
    "                continue\n",
    "            else:\n",
    "                if not current_entry == [None, None, None, None]:\n",
    "                    all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                            segmentation=current_entry[1],\n",
    "                                            glosses=current_entry[2],\n",
    "                                            translation=None))\n",
    "                    current_entry = [None, None, None, None]\n",
    "        # Might have one extra line at the end\n",
    "        if not current_entry == [None, None, None, None]:\n",
    "            all_data.append({\"transcr\"})\n",
    "            all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                    segmentation=current_entry[1],\n",
    "                                    glosses=current_entry[2],\n",
    "                                    translation=None))\n",
    "        print(f\"Skipped {len(skipped_lines)} lines\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a86ca3e0-cd23-45af-af01-63b55a4720a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3261f373a94bfca198d7cd69d29764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/633 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lecslab--glosslm-cc23d567cd684e4f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /Users/milesper/.cache/huggingface/datasets/lecslab___parquet/lecslab--glosslm-cc23d567cd684e4f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58eb464c46054fb58204a317ba86f282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fe952bea8c46a2b4d6a5931def9d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14aa0e027ba488b87555c8ae6f5a31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/116339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /Users/milesper/.cache/huggingface/datasets/lecslab___parquet/lecslab--glosslm-cc23d567cd684e4f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f156aa7ab940a59f228a6fd2953f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "glosslm = datasets.load_dataset(\"lecslab/glosslm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8dcff95-bb8f-4f10-a68e-a79b3c6a0a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./st_data/Arapaho/st_data/arp-CLDFmaster.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Gitksan/st_data\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Lezgi/st_data/lez-CLDFmaster.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Natugu/st_data/ntu.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Nyangbo/st_data/nyb.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Tsez/st_data/ddo-CLDFmaster.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Uspanteko/st_data\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n"
     ]
    }
   ],
   "source": [
    "def create_hf_dataset(filename, glottocode, metalang):\n",
    "    print(f\"Loading {filename}\")\n",
    "    raw_data = load_data_file(filename)\n",
    "    data = []\n",
    "    for i, line in enumerate(raw_data):\n",
    "        new_row = {'glottocode': glottocode, 'metalang_glottocode': metalang, \"is_segmented\": \"yes\", \"source\": \"sigmorphon_st\"}\n",
    "        new_row['ID'] = f\"st_{glottocode}_{i}\"\n",
    "        new_row['transcription'] = line.segmentation\n",
    "        new_row['glosses'] = line.glosses\n",
    "        new_row['translation'] = line.translation\n",
    "        data.append(new_row)\n",
    "\n",
    "        new_row_unsegmented = {'glottocode': glottocode, 'metalang_glottocode': metalang, \"is_segmented\": \"no\", \"source\": \"sigmorphon_st\"}\n",
    "        new_row_unsegmented['ID'] = f\"st_{glottocode}_{i}_unseg\"\n",
    "        new_row_unsegmented['transcription'] = line.transcription\n",
    "        new_row_unsegmented['glosses'] = line.glosses\n",
    "        new_row_unsegmented['translation'] = line.translation\n",
    "        data.append(new_row_unsegmented)\n",
    "\n",
    "    return datasets.Dataset.from_list(data)\n",
    "\n",
    "st_data = {\n",
    "    \"arp\": create_hf_dataset(\"./st_data/Arapaho/st_data/arp-CLDFmaster.txt\", \"arap1274\", \"stan1293\"),\n",
    "    \"git\": create_hf_dataset(\"./st_data/Gitksan/st_data\", \"gitx1241\", \"stan1293\"),\n",
    "    \"lez\": create_hf_dataset(\"./st_data/Lezgi/st_data/lez-CLDFmaster.txt\", \"lezg1247\", \"stan1293\"),\n",
    "    \"nat\": create_hf_dataset(\"./st_data/Natugu/st_data/ntu.txt\", \"natu1246\", \"stan1293\"),\n",
    "    \"nyb\": create_hf_dataset(\"./st_data/Nyangbo/st_data/nyb.txt\", \"nyan1302\", \"stan1293\"),\n",
    "    \"ddo\": create_hf_dataset(\"./st_data/Tsez/st_data/ddo-CLDFmaster.txt\", \"dido1241\", \"stan1293\"),\n",
    "    \"usp\": create_hf_dataset(\"./st_data/Uspanteko/st_data\", \"uspa1245\", \"stan1288\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "835491af-14d3-4209-8b15-ffc2709a37b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arp': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 97832\n",
       " }),\n",
       " 'git': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 220\n",
       " }),\n",
       " 'lez': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 1752\n",
       " }),\n",
       " 'nat': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 1978\n",
       " }),\n",
       " 'nyb': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 5252\n",
       " }),\n",
       " 'ddo': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 8896\n",
       " }),\n",
       " 'usp': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 21278\n",
       " })}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da42bce8-907d-4d30-a947-d618f1a4112e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d94096d26e743f2879a98aebd98a587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b9ed2ec53640c09e0420d48d82630b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ec6fbc5a0e40c08ad0526f62f91691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "combined = datasets.concatenate_datasets([glosslm['train']] + list(st_data.values()))\n",
    "combined.push_to_hub(\"lecslab/glosslm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e3e41-fd85-41dc-8712-94c0fab46d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
