{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b720a8f9-a47b-4a1f-bdf1-432beaf77a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from st_data.data import IGTLine\n",
    "\n",
    "def load_data_file(path: str):\n",
    "    \"\"\"Loads a file containing IGT data into a list of entries.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # If we have a directory, recursively load all files and concat together\n",
    "    if os.path.isdir(path):\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                all_data.extend(load_data_file(os.path.join(path, file)))\n",
    "        return all_data\n",
    "\n",
    "    # If we have one file, read in line by line\n",
    "    with open(path, 'r') as file:\n",
    "        current_entry = [None, None, None, None]  # transc, segm, gloss, transl\n",
    "\n",
    "        skipped_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            # Determine the type of line\n",
    "            # If we see a type that has already been filled for the current entry, something is wrong\n",
    "            line_prefix = line[:2]\n",
    "            if line_prefix == '\\\\t' and current_entry[0] == None:\n",
    "                current_entry[0] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\m' and current_entry[1] == None:\n",
    "                current_entry[1] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\g' and current_entry[2] == None:\n",
    "                if len(line[3:].strip()) > 0:\n",
    "                    current_entry[2] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\l' and current_entry[3] == None:\n",
    "                current_entry[3] = line[3:].strip()\n",
    "                # Once we have the translation, we've reached the end and can save this entry\n",
    "                all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                        segmentation=current_entry[1],\n",
    "                                        glosses=current_entry[2],\n",
    "                                        translation=current_entry[3]))\n",
    "                current_entry = [None, None, None, None]\n",
    "            elif line_prefix == \"\\\\p\":\n",
    "                # Skip POS lines\n",
    "                continue\n",
    "            elif line.strip() != \"\":\n",
    "                # Something went wrong\n",
    "                skipped_lines.append(line)\n",
    "                continue\n",
    "            else:\n",
    "                if not current_entry == [None, None, None, None]:\n",
    "                    all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                            segmentation=current_entry[1],\n",
    "                                            glosses=current_entry[2],\n",
    "                                            translation=None))\n",
    "                    current_entry = [None, None, None, None]\n",
    "        # Might have one extra line at the end\n",
    "        if not current_entry == [None, None, None, None]:\n",
    "            all_data.append({\"transcr\"})\n",
    "            all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                    segmentation=current_entry[1],\n",
    "                                    glosses=current_entry[2],\n",
    "                                    translation=None))\n",
    "        print(f\"Skipped {len(skipped_lines)} lines\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86ca3e0-cd23-45af-af01-63b55a4720a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8033b824004e4efdaa934a794efe9ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6857502a87d4e2398fd83193f7a7a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c8fc02530548529310ec0a44a781b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/31.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827a4cf304f34c34a96a4883d42d57a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7acaaa8a14410da2b10788bf7453c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/451407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "glosslm = datasets.load_dataset(\"lecslab/glosslm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8dcff95-bb8f-4f10-a68e-a79b3c6a0a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./st_data/Arapaho/st_data/arp-CLDFmaster.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Gitksan/st_data\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Lezgi/st_data/lez-CLDFmaster.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Natugu/st_data/ntu.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Nyangbo/st_data/nyb.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Tsez/st_data/ddo-CLDFmaster.txt\n",
      "Skipped 0 lines\n",
      "Loading ./st_data/Uspanteko/st_data\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n"
     ]
    }
   ],
   "source": [
    "def create_hf_dataset(filename, glottocode, metalang):\n",
    "    print(f\"Loading {filename}\")\n",
    "    raw_data = load_data_file(filename)\n",
    "    data = []\n",
    "    for i, line in enumerate(raw_data):\n",
    "        new_row = {'glottocode': glottocode, 'metalang_glottocode': metalang, \"is_segmented\": \"yes\", \"source\": \"sigmorphon_st\", \"type\": \"canonical\"}\n",
    "        new_row['ID'] = f\"st_{glottocode}_{i}\"\n",
    "        new_row['transcription'] = line.segmentation\n",
    "        new_row['glosses'] = line.glosses\n",
    "        new_row['translation'] = line.translation\n",
    "        data.append(new_row)\n",
    "\n",
    "        new_row_unsegmented = {'glottocode': glottocode, 'metalang_glottocode': metalang, \"is_segmented\": \"no\", \"source\": \"sigmorphon_st\", \"type\": \"canonical\"}\n",
    "        new_row_unsegmented['ID'] = f\"st_{glottocode}_{i}\"\n",
    "        new_row_unsegmented['transcription'] = line.transcription\n",
    "        new_row_unsegmented['glosses'] = line.glosses\n",
    "        new_row_unsegmented['translation'] = line.translation\n",
    "        data.append(new_row_unsegmented)\n",
    "\n",
    "    return datasets.Dataset.from_list(data)\n",
    "\n",
    "st_data = {\n",
    "    \"arp\": create_hf_dataset(\"./st_data/Arapaho/st_data/arp-CLDFmaster.txt\", \"arap1274\", \"stan1293\"),\n",
    "    \"git\": create_hf_dataset(\"./st_data/Gitksan/st_data\", \"gitx1241\", \"stan1293\"),\n",
    "    \"lez\": create_hf_dataset(\"./st_data/Lezgi/st_data/lez-CLDFmaster.txt\", \"lezg1247\", \"stan1293\"),\n",
    "    \"nat\": create_hf_dataset(\"./st_data/Natugu/st_data/ntu.txt\", \"natu1246\", \"stan1293\"),\n",
    "    \"nyb\": create_hf_dataset(\"./st_data/Nyangbo/st_data/nyb.txt\", \"nyan1302\", \"stan1293\"),\n",
    "    \"ddo\": create_hf_dataset(\"./st_data/Tsez/st_data/ddo-CLDFmaster.txt\", \"dido1241\", \"stan1293\"),\n",
    "    \"usp\": create_hf_dataset(\"./st_data/Uspanteko/st_data\", \"uspa1245\", \"stan1288\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "835491af-14d3-4209-8b15-ffc2709a37b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arp': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 48916\n",
       " }),\n",
       " 'git': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 110\n",
       " }),\n",
       " 'lez': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 876\n",
       " }),\n",
       " 'nat': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 989\n",
       " }),\n",
       " 'nyb': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 2626\n",
       " }),\n",
       " 'ddo': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 4448\n",
       " }),\n",
       " 'usp': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 10639\n",
       " })}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da42bce8-907d-4d30-a947-d618f1a4112e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7382cd6eebe4d2c9e53f581b48c639b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7840180de5ef4343a7b2718a4ae9c8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3578d0df0be476b841318f31ce963c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "combined = datasets.concatenate_datasets([glosslm['train']] + list(st_data.values()))\n",
    "combined.push_to_hub(\"lecslab/glosslm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d82cbfb-5c82-4f0a-9963-164884f94209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3cc8036f64405ca1b6c1d45fac1e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982bd91f51e44cacad50be56a7ca6662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a00700df15d4f67b5a7042fe9c54889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46494547a5de495aa3265a9590e0aca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/714 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc71d1e6-29a6-491e-8b91-99b0cb5777a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2980228ccd4c46b4b5982bb9e26bac26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c94e09dffc64eacac167d3092bd9b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f03a3647a84196a62052bc81d0db06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5afa919cd7243ceadae41cb7ba24aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ea46ab24a249c0baa561916268cb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4ceaea17eb4bdea7196517719a49c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9931148c7b249f4a7b0bb896941f287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ccfcc87f084704bba278f3ae0465a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "train_test = st_data['usp'].train_test_split(test_size=0.3)\n",
    "test_val = train_test['test'].train_test_split(test_size=0.5)\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_val['test'],\n",
    "    'valid': test_val['train']})\n",
    "\n",
    "# unsegmented_train = train_test_valid_dataset['train'].copy()\n",
    "\n",
    "# dataset_list = [item for item in train_test_valid_dataset['train']]\n",
    "# duplicated_dataset_list = dataset_list * 2\n",
    "# for i in range(len(dataset_list), len(duplicated_dataset_list)):\n",
    "#     duplicated_dataset_list[i]['is_s'] = (duplicated_dataset_list[i]['label'] + 1) % 2  # Flip label for demonstration\n",
    "\n",
    "train_test_valid_dataset.push_to_hub(\"lecslab/usp-igt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6516dd86-f5d7-4257-af48-c2932f81674f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "        num_rows: 7447\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'type', 'ID', 'transcription', 'glosses', 'translation'],\n",
       "        num_rows: 3192\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1e7ee-5baa-4628-8ec1-822e5c9fa95a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
