{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b720a8f9-a47b-4a1f-bdf1-432beaf77a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data import IGTLine\n",
    "\n",
    "def load_data_file(path: str):\n",
    "    \"\"\"Loads a file containing IGT data into a list of entries.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # If we have a directory, recursively load all files and concat together\n",
    "    if os.path.isdir(path):\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                all_data.extend(load_data_file(os.path.join(path, file)))\n",
    "        return all_data\n",
    "\n",
    "    # If we have one file, read in line by line\n",
    "    with open(path, 'r') as file:\n",
    "        current_entry = [None, None, None, None]  # transc, segm, gloss, transl\n",
    "\n",
    "        skipped_lines = []\n",
    "        \n",
    "        for line in file:\n",
    "            # Determine the type of line\n",
    "            # If we see a type that has already been filled for the current entry, something is wrong\n",
    "            line_prefix = line[:2]\n",
    "            if line_prefix == '\\\\t' and current_entry[0] == None:\n",
    "                current_entry[0] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\m' and current_entry[1] == None:\n",
    "                current_entry[1] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\g' and current_entry[2] == None:\n",
    "                if len(line[3:].strip()) > 0:\n",
    "                    current_entry[2] = line[3:].strip()\n",
    "            elif line_prefix == '\\\\l' and current_entry[3] == None:\n",
    "                current_entry[3] = line[3:].strip()\n",
    "                # Once we have the translation, we've reached the end and can save this entry\n",
    "                all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                        segmentation=current_entry[1],\n",
    "                                        glosses=current_entry[2],\n",
    "                                        translation=current_entry[3]))\n",
    "                current_entry = [None, None, None, None]\n",
    "            elif line_prefix == \"\\\\p\":\n",
    "                # Skip POS lines\n",
    "                continue\n",
    "            elif line.strip() != \"\":\n",
    "                # Something went wrong\n",
    "                skipped_lines.append(line)\n",
    "                continue\n",
    "            else:\n",
    "                if not current_entry == [None, None, None, None]:\n",
    "                    all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                            segmentation=current_entry[1],\n",
    "                                            glosses=current_entry[2],\n",
    "                                            translation=None))\n",
    "                    current_entry = [None, None, None, None]\n",
    "        # Might have one extra line at the end\n",
    "        if not current_entry == [None, None, None, None]:\n",
    "            all_data.append(IGTLine(transcription=current_entry[0],\n",
    "                                    segmentation=current_entry[1],\n",
    "                                    glosses=current_entry[2],\n",
    "                                    translation=None))\n",
    "        print(f\"Skipped {len(skipped_lines)} lines\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86ca3e0-cd23-45af-af01-63b55a4720a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26573875c4243cf874f2bb97e7bf48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038781d7a1e843e5be2ccf08f997f1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d113ce320a4539bb6827c9686475dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/22.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d66efef12b4de391251571d0c0b558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e95fc3fb124edc89aac75eb040c50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/312294 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "glosslm = datasets.load_dataset(\"lecslab/glosslm\", download_mode='force_redownload', verification_mode=\"no_checks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dcff95-bb8f-4f10-a68e-a79b3c6a0a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/st_data/splits/Arapaho\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ../data/st_data/splits/Gitksan\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ../data/st_data/splits/Lezgi\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ../data/st_data/splits/Natugu\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ../data/st_data/splits/Nyangbo\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ../data/st_data/splits/Tsez\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Loading ../data/st_data/splits/Uspanteko\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n",
      "Skipped 0 lines\n"
     ]
    }
   ],
   "source": [
    "def create_hf_dataset(filename, isocode, glottocode, metalang):\n",
    "    print(f\"Loading {filename}\")\n",
    "    train_data = load_data_file(filename + f\"/{isocode}-train-track2-uncovered\")\n",
    "    dev_data = load_data_file(filename + f\"/{isocode}-dev-track2-uncovered\")\n",
    "    test_data = load_data_file(filename + f\"/{isocode}-test-track2-uncovered\")\n",
    "    \n",
    "    def parse_data(raw_data, id_prefix: str):\n",
    "        data = []\n",
    "        for i, line in enumerate(raw_data):\n",
    "            new_row = {'glottocode': glottocode, 'metalang_glottocode': metalang, \"is_segmented\": \"yes\", \"source\": \"sigmorphon_st\"}\n",
    "            new_row['id'] = f\"st_{id_prefix}_{glottocode}_{i}\"\n",
    "            new_row['transcription'] = line.segmentation\n",
    "            new_row['glosses'] = line.glosses\n",
    "            new_row['translation'] = line.translation\n",
    "            data.append(new_row)\n",
    "\n",
    "            new_row_unsegmented = {'glottocode': glottocode, 'metalang_glottocode': metalang, \"is_segmented\": \"no\", \"source\": \"sigmorphon_st\"}\n",
    "            new_row_unsegmented['id'] = f\"st_{id_prefix}_{glottocode}_{i}\"\n",
    "            new_row_unsegmented['transcription'] = line.transcription\n",
    "            new_row_unsegmented['glosses'] = new_row['glosses']\n",
    "            new_row_unsegmented['translation'] = line.translation\n",
    "            data.append(new_row_unsegmented)\n",
    "        return data\n",
    "    \n",
    "    data = parse_data(train_data, 'train') + parse_data(dev_data, 'dev') + parse_data(test_data, 'test')\n",
    "\n",
    "    return datasets.Dataset.from_list(data)\n",
    "\n",
    "st_data = {\n",
    "    \"arp\": create_hf_dataset(\"../data/st_data/splits/Arapaho\", \"arp\", \"arap1274\", \"stan1293\"),\n",
    "    \"git\": create_hf_dataset(\"../data/st_data/splits/Gitksan\", \"git\", \"gitx1241\", \"stan1293\"),\n",
    "    \"lez\": create_hf_dataset(\"../data/st_data/splits/Lezgi\", \"lez\", \"lezg1247\", \"stan1293\"),\n",
    "    \"nat\": create_hf_dataset(\"../data/st_data/splits/Natugu\", \"ntu\", \"natu1246\", \"stan1293\"),\n",
    "    \"nyb\": create_hf_dataset(\"../data/st_data/splits/Nyangbo\", \"nyb\", \"nyan1302\", \"stan1293\"),\n",
    "    \"ddo\": create_hf_dataset(\"../data/st_data/splits/Tsez\", \"ddo\", \"dido1241\", \"stan1293\"),\n",
    "    \"usp\": create_hf_dataset(\"../data/st_data/splits/Uspanteko\", \"usp\", \"uspa1245\", \"stan1288\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835491af-14d3-4209-8b15-ffc2709a37b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arp': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'id', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 97832\n",
       " }),\n",
       " 'git': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'id', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 220\n",
       " }),\n",
       " 'lez': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'id', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 1752\n",
       " }),\n",
       " 'nat': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'id', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 1978\n",
       " }),\n",
       " 'nyb': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'id', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 5252\n",
       " }),\n",
       " 'ddo': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'id', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 8896\n",
       " }),\n",
       " 'usp': Dataset({\n",
       "     features: ['glottocode', 'metalang_glottocode', 'is_segmented', 'source', 'id', 'transcription', 'glosses', 'translation'],\n",
       "     num_rows: 21278\n",
       " })}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da42bce8-907d-4d30-a947-d618f1a4112e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7382cd6eebe4d2c9e53f581b48c639b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7840180de5ef4343a7b2718a4ae9c8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3578d0df0be476b841318f31ce963c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "# combined = datasets.concatenate_datasets([glosslm['train']] + list(st_data.values()))\n",
    "# combined.push_to_hub(\"lecslab/glosslm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e3e61c-512c-4713-96cb-a55455dc7171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glottocode</th>\n",
       "      <th>metalang_glottocode</th>\n",
       "      <th>is_segmented</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>transcription</th>\n",
       "      <th>glosses</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arap1274</td>\n",
       "      <td>stan1293</td>\n",
       "      <td>yes</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_train_arap1274_0</td>\n",
       "      <td>wootii niiyou heesi-ini hee3ohwoo-ni3 'oh hih-...</td>\n",
       "      <td>like here.it.is what-DETACH how.s.o..is.dancin...</td>\n",
       "      <td>I guess the way he was dancing , they had neve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arap1274</td>\n",
       "      <td>stan1293</td>\n",
       "      <td>no</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_train_arap1274_0</td>\n",
       "      <td>wootii niiyou heesiini hee3ohwooni3 'oh hih'ow...</td>\n",
       "      <td>like here.it.is what-DETACH how.s.o..is.dancin...</td>\n",
       "      <td>I guess the way he was dancing , they had neve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arap1274</td>\n",
       "      <td>stan1293</td>\n",
       "      <td>yes</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_train_arap1274_1</td>\n",
       "      <td>'oh siiyeih hiiwoonhehe' hoowooh-'uni</td>\n",
       "      <td>but INTENSE now no.longer-DETACH</td>\n",
       "      <td>But today that's really gone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arap1274</td>\n",
       "      <td>stan1293</td>\n",
       "      <td>no</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_train_arap1274_1</td>\n",
       "      <td>'oh siiyeih hiiwoonhehe' hoowooh'uni</td>\n",
       "      <td>but INTENSE now no.longer-DETACH</td>\n",
       "      <td>But today that's really gone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arap1274</td>\n",
       "      <td>stan1293</td>\n",
       "      <td>yes</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_train_arap1274_2</td>\n",
       "      <td>nih-tonoun-owoo biii-no' noh hoote</td>\n",
       "      <td>PAST-use-1S plume-NA.PL and sinew</td>\n",
       "      <td>I used feathers and sinew .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137203</th>\n",
       "      <td>uspa1245</td>\n",
       "      <td>stan1288</td>\n",
       "      <td>no</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_test_uspa1245_630</td>\n",
       "      <td>loke tren jun kristyan re jun kristyan .</td>\n",
       "      <td>loque INC-E3S-hacer uno persona PART uno persona</td>\n",
       "      <td>Loque hace una persona a una otra persona.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137204</th>\n",
       "      <td>uspa1245</td>\n",
       "      <td>stan1288</td>\n",
       "      <td>yes</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_test_uspa1245_631</td>\n",
       "      <td>syempr ti-j-toj na loke t-r-en re</td>\n",
       "      <td>siempre INC-E3S-pagar PART loque INC-E3S-hacer...</td>\n",
       "      <td>Siempre tiene que pagar loque uno hace.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137205</th>\n",
       "      <td>uspa1245</td>\n",
       "      <td>stan1288</td>\n",
       "      <td>no</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_test_uspa1245_631</td>\n",
       "      <td>Syempr tijtoj na loke tren re .</td>\n",
       "      <td>siempre INC-E3S-pagar PART loque INC-E3S-hacer...</td>\n",
       "      <td>Siempre tiene que pagar loque uno hace.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137206</th>\n",
       "      <td>uspa1245</td>\n",
       "      <td>stan1288</td>\n",
       "      <td>yes</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_test_uspa1245_632</td>\n",
       "      <td>ri' li t-an-b'ij</td>\n",
       "      <td>DEM DEM INC-E1S-decir</td>\n",
       "      <td>Eso es lo que digo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137207</th>\n",
       "      <td>uspa1245</td>\n",
       "      <td>stan1288</td>\n",
       "      <td>no</td>\n",
       "      <td>sigmorphon_st</td>\n",
       "      <td>st_test_uspa1245_632</td>\n",
       "      <td>Ri' li tanb'ij .</td>\n",
       "      <td>DEM DEM INC-E1S-decir</td>\n",
       "      <td>Eso es lo que digo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137208 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       glottocode metalang_glottocode is_segmented         source  \\\n",
       "0        arap1274            stan1293          yes  sigmorphon_st   \n",
       "1        arap1274            stan1293           no  sigmorphon_st   \n",
       "2        arap1274            stan1293          yes  sigmorphon_st   \n",
       "3        arap1274            stan1293           no  sigmorphon_st   \n",
       "4        arap1274            stan1293          yes  sigmorphon_st   \n",
       "...           ...                 ...          ...            ...   \n",
       "137203   uspa1245            stan1288           no  sigmorphon_st   \n",
       "137204   uspa1245            stan1288          yes  sigmorphon_st   \n",
       "137205   uspa1245            stan1288           no  sigmorphon_st   \n",
       "137206   uspa1245            stan1288          yes  sigmorphon_st   \n",
       "137207   uspa1245            stan1288           no  sigmorphon_st   \n",
       "\n",
       "                          id  \\\n",
       "0        st_train_arap1274_0   \n",
       "1        st_train_arap1274_0   \n",
       "2        st_train_arap1274_1   \n",
       "3        st_train_arap1274_1   \n",
       "4        st_train_arap1274_2   \n",
       "...                      ...   \n",
       "137203  st_test_uspa1245_630   \n",
       "137204  st_test_uspa1245_631   \n",
       "137205  st_test_uspa1245_631   \n",
       "137206  st_test_uspa1245_632   \n",
       "137207  st_test_uspa1245_632   \n",
       "\n",
       "                                            transcription  \\\n",
       "0       wootii niiyou heesi-ini hee3ohwoo-ni3 'oh hih-...   \n",
       "1       wootii niiyou heesiini hee3ohwooni3 'oh hih'ow...   \n",
       "2                   'oh siiyeih hiiwoonhehe' hoowooh-'uni   \n",
       "3                    'oh siiyeih hiiwoonhehe' hoowooh'uni   \n",
       "4                      nih-tonoun-owoo biii-no' noh hoote   \n",
       "...                                                   ...   \n",
       "137203           loke tren jun kristyan re jun kristyan .   \n",
       "137204                  syempr ti-j-toj na loke t-r-en re   \n",
       "137205                    Syempr tijtoj na loke tren re .   \n",
       "137206                                   ri' li t-an-b'ij   \n",
       "137207                                   Ri' li tanb'ij .   \n",
       "\n",
       "                                                  glosses  \\\n",
       "0       like here.it.is what-DETACH how.s.o..is.dancin...   \n",
       "1       like here.it.is what-DETACH how.s.o..is.dancin...   \n",
       "2                        but INTENSE now no.longer-DETACH   \n",
       "3                        but INTENSE now no.longer-DETACH   \n",
       "4                       PAST-use-1S plume-NA.PL and sinew   \n",
       "...                                                   ...   \n",
       "137203   loque INC-E3S-hacer uno persona PART uno persona   \n",
       "137204  siempre INC-E3S-pagar PART loque INC-E3S-hacer...   \n",
       "137205  siempre INC-E3S-pagar PART loque INC-E3S-hacer...   \n",
       "137206                              DEM DEM INC-E1S-decir   \n",
       "137207                              DEM DEM INC-E1S-decir   \n",
       "\n",
       "                                              translation  \n",
       "0       I guess the way he was dancing , they had neve...  \n",
       "1       I guess the way he was dancing , they had neve...  \n",
       "2                          But today that's really gone .  \n",
       "3                          But today that's really gone .  \n",
       "4                             I used feathers and sinew .  \n",
       "...                                                   ...  \n",
       "137203         Loque hace una persona a una otra persona.  \n",
       "137204            Siempre tiene que pagar loque uno hace.  \n",
       "137205            Siempre tiene que pagar loque uno hace.  \n",
       "137206                                Eso es lo que digo.  \n",
       "137207                                Eso es lo que digo.  \n",
       "\n",
       "[137208 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace data on hf\n",
    "\n",
    "st_data_rows = datasets.concatenate_datasets(list(st_data.values())).to_pandas()\n",
    "st_data_rows['transcription'] = st_data_rows['transcription'] \\\n",
    "            .str.replace('\\t', ' ') \\\n",
    "            .str.replace(r\"(\\w)\\?\", r\"\\1 ?\", regex=True) \\\n",
    "            .str.replace(r\"(\\w)\\.\", r\"\\1 .\", regex=True) \\\n",
    "            .str.replace(r\"(\\w)\\!\", r\"\\1 !\", regex=True) \\\n",
    "            .str.replace(r\"(\\w)\\,\", r\"\\1 ,\", regex=True) \\\n",
    "            .str.replace(\"\\-(\\s|$)\", \" \", regex=True)\n",
    "\n",
    "st_data_rows['glosses'] = st_data_rows['glosses'] \\\n",
    "            .str.replace(\"\\t\", \" \") \\\n",
    "            .str.replace(\"\\-(\\s|$)\", \" \", regex=True) \\\n",
    "            .str.replace(r\"(\\w)\\.(\\s|$)\", r\"\\1 . \", regex=True) \\\n",
    "            .str.replace(r\"(\\w)\\!(\\s|$)\", r\"\\1 ! \", regex=True) \\\n",
    "            .str.replace(r\"(\\w)\\?(\\s|$)\", r\"\\1 ? \", regex=True)\n",
    "st_data_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc14272-ea5e-49e5-9cf8-c3229375049d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['transcription', 'glosses', 'translation', 'glottocode', 'id', 'source', 'metalang_glottocode', 'is_segmented'],\n",
       "    num_rows: 451108\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data = glosslm['train'].to_pandas()\n",
    "old_data = old_data[old_data['source'] != \"sigmorphon_st\"]\n",
    "a = pd.concat([old_data, st_data_rows])\n",
    "ds = datasets.Dataset.from_pandas(a).remove_columns([\"__index_level_0__\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00af61f-311b-4de2-b557-3fd53dc1cebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f5c13a6f334270b1a72d61f99808e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6730d898687b4291a4a930b9c9e5055f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/452 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01b0f5a99ea4b5993f9ff6b727ba6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.push_to_hub(\"lecslab/glosslm\", commit_message='Add ST data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf4544-62dd-467b-af2b-4d19dd2aa62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
